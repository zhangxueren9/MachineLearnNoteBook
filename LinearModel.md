1.监督学习和无监督学习
根据训练数据是否含有标记(标签或数值)，可将机器学习任务分为有监督学习和无监督学习。
回归和分类问题是最常见的监督学习，聚类问题是常见的无监督学习。
如：根据房屋属性(面积、楼层、街道、城市、房龄等)预测房价，训练数据中可以拿到房价，模型需要预测的也是房价，这就是典型的监督学习(回归问题)；
再如：对大量文本进行主题分析，在训练数据中我们只知道文本内容，不知道文本属于哪个主题，甚至连主题是什么(由那些词组成)都无从知晓，这就是典型的无监督学习。

2.泛化能力和过拟合问题
所谓泛化能力就是指模型对未知样本的预测能力，未知样本是指未在训练数据集中出现的样本。
在模型训练过程中，是无法判断模型的泛化能力的。我们会将训练数据分为训练集和测试集。用模型在测试集上的预测能力(准确率、找回率等)估计模型的泛化能力。
当模型在测试集准确率较低时，模型的泛化能力与模型在测试集上的预测能力是正相关的，即模型在测试集表现越好，其泛化能力越强。
但当模型在测试集上准确率较高时，模型的泛化能力与模型在测试集上的预测能力呈现负相关，即模型在测试集表现非常好，但其泛化能力较差，这种情况即为过拟合现象。
过拟合现象是无法完全避免的，但我们可以通过缩小样本数量，交叉验证，集成多个模型(随机森林和xgboost)等方式缓解过拟合。
对于线性模型，该模型本身对数据的拟合能力较差，除非经过大量特征过程，否则可不必考虑过拟合问题。
造成过拟合的原因是由于对规律的拟合能力太强(个人造的名词)。
线性模型、未经过核方法进行维度处理的svm其拟合能力较差，但神经网络，不考虑剪枝的决策树拟合能力都很强。

3.线性回归原理
线性回归假设所有最终结果是由不同的特征加权求和得到的。不同的特征其权重不同，有正有负，有大有小。对应实际问题，即不同因素对最终结果有不同的影响程度。
如房价预测模型，面积、楼层、房龄，面积对房屋价格的影响大于楼层和房龄，面积越大价格越高，因此在模型中，面积的权重为较大的正数，
房龄对房屋的价格影响是负面的，且影响相对面积较小，因此在模型中，房龄的权重为绝对值较小的负数。

4.线性模型损失函数
线性模型损失函数为： 预测值与真实值之差的平方和 / (2*训练集样本个数)
为了防止过拟合(通常意义不是很大)：我们会添加正则化项：特征权重的平方和×系数 通过调整系数来控制模型特征个数，
这样可以避免我们的模型过于复杂(考虑的因素太多)导致过拟合。

5.梯度下降
有了损失函数和模型,我们就有了训练的目标。训练过程的目标就是找到一组参数使得模型在训练集上的误差最小。
将参数和损失函数的值看做一个曲面，那么训练的任务就是找到这个曲面上的最小值，可以看做是一个凹凸不平的碗，训练的过程就是不断的尝试，并最终走到碗底。
(很多时候未必能走到碗底，因为实际问题并非凸优化问题啊，在下降的路上有无数的坑)，
在这个充满坑的碗中，我每次前进的距离是固定的(学习率),那么我如何才能走到碗底呢？
梯度下降的方法是：始终沿着当前位置下降最快的方向前进。这个方向就是数学上梯度的反方向。
在不断前进的过程中，如何判断是否已经到达碗底呢？
没走一步，我会比较一下是否下降的距离，不断的走，下降的距离几乎不变时，即认为已经走到碗底。
训练过程结束，此时的参数作为模型参数，此时，就拿到了模型。




参考资料：无
